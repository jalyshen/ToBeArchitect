# TF-IDF介绍

原文： https://easyai.tech/ai-definition/tf-idf/

## 什么是TF-IDF

​        简单来说，**向量空间模型就是希望把查询关键字和文档都表达成向量，然后利用向量之间的运算来进一步表达向量间的关系**。比如，一个比较常用的运算就是计算查询关键字所对应的向量和文档所对应的向量之间的 “**相关度**”。

![](./images/TF-IDF-simple-define.png)



### TF(Term Frequency) - 单词频率

​        TF，简单的说，就是计算一个查询关键字中某一个单词在目标文档中出现的次数。

举个例子，比如我们要查询“Car Insurance”，那么对于每一个文档，我们都计算“Car”这个单词出现了多少次，“Insurance”这个单词在其中出现了多少次。这就是TF的计算方法。 

​        TF背后的隐含假设是，查询关键字中单词应该相对于其他单词更加重要，而文档的重要程度，也就是相关度，与单词在文档中出现的次数成正比。比如：“Car”这个词在文档A中出现了5次，而在文档B中出现了20次，那么TF计算就认为文档B可能更相关。

​        然而，信息检索工作者很快就发现，仅有TF不能比较完整地描述文档的相关度。因为语言的因素，一些单词可能会比较自然的在很多文档中反复出现，比如英语中的“The”、“An”，“But”等等。这些词大多起到了连接语句的作用，是保持语言连贯不可或缺的部分。然而，如果我们去搜索“How to build a car”这个关键词，其中的“How”、“To”以及“A”都极可能在绝大多数的文档中出现，这个时候，TF就无法帮助我们区分文档的相关度了。

​        通过上面的说明，应该能理解TF的含义了。现在通过一些数学表达式进行归纳。

        1. 假设，一个查询包含$$N$$个关键词： $$w_1, w_2, ... , w_N$$
           2. 这些关键词在一个特定的文档中的词频分别是： $$TF_1, TF_2, ... , TF_N$$
           3. 这个查询和该文档的相关性就是： $$TF = \displaystyle \sum^{N}_{n \to 1}{TF_n}$$   - <font color='red'>*公式 1*</font>

### IDF(Inverse Document Frequency) - 逆文档频率

​        显然，IDF就是为了去除那么出现在太多文档中的单词。

​        也就是说，真正携带“相关”信息的单词仅仅出现在相对少、有时候可能是极少数的文档里。这个信息，很容易用“**文档频率**”来计算，**即“有多少文档涵盖了这个词”**。显然，如果有太多文档都涵盖了某个词，这个词也就越不重要，或者说是这个词就越没有信息量。因此，需要对TF的值进行修正。而IDF的想法是用DF的**倒数**来进行修正。倒数的应用，正好表达了这样的思想：DF值越大越不重要。

> *注：TF-IDF算法主要适用于英文，中文首先要分词，分词后要解决多词一义，以及一词多义问题。这两个问题通过简单的TF-IDF是不能解决的。于是就有了后来的**词嵌入方法**，用向量来表征一个词。*

​        上面的表述，用数学公式来表达，应该是这样的：

1. 假设，一共有 $$D$$ 个产品的说明文档
2. 要搜索的关键词是$$ w $$，含有这个词$$w$$的文档有 $$D_w$$ 个
3. 则这个关键词$$w$$的IDF值：  $$IDF_w = \displaystyle \frac{D}{D_w}$$     - <font color='red'>*公式2*</font>

## TF-IDF的4个变种

![](./images/if-idf-4.png)



> *修正：*
>
> 1. *变种1 -> 解决 TF 现行增长问题    =>  解决 TF **线性**增长问题*
> 2. *变种3 -> 解决 IDF 现行增长问题 => 解决 IDF **线性**增加问题*

### 变种一：通过对数函数避免TF线性增长

​       很多人注意到TF的值在原始的定义中没有任何上限。虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但是这样的关系很难说是线性的。刚才的例子“Car Insurance”，文档A可能包含“Car”这个词100次，而文档B可能包含200次，是不是说文档B的相关度就是文档A的2倍呢？其实很多人意识到，超过了某个阈值之后，这个TF就没有那么有区分度了。

​       使用对数函数，对TF进行变换，就是一个不让TF线性增长的技巧。具体来说，人们常用 ***1 + Log(TF)*** 这个值来代替原来的TF的值。在这样的计算下，假设“Car”出现一次，新的值是1， 出现100次，新的值是5.6，而出现200次，新的值是6.3。可以看出，这样的计算保持了一个平很，既有区分度，又不至于完全线性增长。

> *注：为啥蝉唱使用对数函数呢？因为对数函数既可以抑制线性增长，同时还能保持原有函数增长的趋势*

​        上面的 <font color='red'>*公式1* </font>要变化为<font color='red'>*公式3*</font>：
$$
TF_{w-new} = 1 + \log(\displaystyle \sum^{N}_{n \to 1}{TF_n})
$$
​    

### 变种三：对数函数处理IDF，避免线性增长

​       对IDF也进行对数处理，原理同TF。相对于直接使用 IDF 来作为 “惩罚因素”，我们可以使用 ***D*** **除以** ***DF+1*** 作为一个***新的 DF 的倒数***，并且再在这个基础上通过一个对数变化。这里的 ***D 是所有文档的总数***。这样做的好处就是：

* 第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路
* 第二，利用对数来达到非线性增长的目的

​        新的IDF的值，可以用这个式子表达， <font color='red'>*公式2* </font>变为<font color='red'>*公式4*</font>： 
$$
IDF_{new} = \log{\displaystyle \frac{D}{DF + 1} }
$$

### 变种二：标准化解决长文档、短文档问题

​       经典的计算并没有考虑“长文档”和“短文档”的区别。一个文档A有3000个单词，文档B有250个单词。很明显，即便 “Car” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。对TF进行**“标准化”（Normalization）**，特别是根据文档的最大TF值进行的标准化，成为了另外一个比较常用的技巧。

> *注：*如何标准化TF的值呢？ 举个例子：
>
> ​        要查询“iPhone12 Pro Max”，这个查询会被拆分成3个词“iPhone12”、“Pro”、“Max”。在我们的产品库中，假设有一件产品的介绍是关于iPhone12 Pro Max的，而且文中共有1000个词。其中，这3个词分别出现了2次、35次、5次，那么这3个词的词频分别是0.002、0.035和0.005。
>
> ​        现在，将这3个词频值相加得0.042，这个0.042就是这件产品与用户输入内容的标准化后的“单文本词频”。    



### 变种四：查询词及文档向量标准化

​       还有一个重要的 TF-IDF 变种，则是对查询关键字向量，以及文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。在线性代数里，可以把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。所以，另外一个角度利用这个规则就是直接在多数时候进行余弦相似度运算，以代替点积运算。

​        采用线性代数方式，我们后面补充。



## 总结

​       结合变种一、三，从 <font color='red'>*公式3、公式4*</font> 得到标准化的加权求和 <font color='red'>*公式5*</font> ：
$$
IF-IDF_{searching} = \displaystyle \sum^{N}_{n \to 1}{TF_n} \cdot {IDF_n}
$$
​       举个例子来说明当前公式5为什么更符合我们对搜索结果直觉的一致性。

​       上面的例子中，我们使用的是“iPhone12 Pro Max”这个短语，现在继续使用它作为例子。假设，我们的产品数量有$$300,000$$个，即$$D = 300,000$$，“iPhone12” 出现在$$1,000$$个产品文档中，即$$D_w = 1,000$$ , 则“iPhone12”的权重 $$IDF_{iphone12} = \displaystyle \log{\frac{300,000}{1,000 + 1}} \approx 2.4767$$； “Pro”出现在$$5,00$$ 个产品文档中，同样得$$IDF_{Pro} = \displaystyle \log{\frac{300,000}{500 + 1}} \approx 2.7773$$；“Max”出现在$$200$$个产品文档中，得$$IDF_{Max} = \displaystyle \log{\frac{300,000}{200 + 1}} \approx 3.1739$$。

​       最后，在上面举例中，我们假设这3个词在某一个产品文档中的IF分别是0.002, 0.035, 0.005， 再根据公式5，得到：
$$
IF-IDF_{input} = 0.002 * 2.4767 + 0.035 * 2.7773 + 0.005 * 3.1739 = 0.005 + 0.097 + 0.016 \approx 0.118
$$
这说明，当前的产品与用户搜索内容的相关性为 $$0.118$$，其中，“iPhone”贡献了0.005，"Pro"贡献了“0.097”，“Max”贡献了0.016。



