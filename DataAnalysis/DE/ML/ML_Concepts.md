# 机器学习核心概念

原文：https://www.toutiao.com/a6901217178578878979/



刚接触机器学习框架 TensorFlow 的新手们，这篇由 Google 官方出品的常用术语词汇表，一定是你必不可少的入门资料！本术语表列出了基本的机器学习术语和 TensorFlow 专用术语的定义，希望能帮助您快速熟悉 TensorFlow 入门内容，轻松打开机器学习世界的大门。

本文来源：

https://developers.google.cn/machine-learning/glossary?hl=zh-CN



## A

### A/B测试 (A/B Testing)

​        一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方法对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。

### 准确率 (Accuracy)

​        分类模型的正确预测所占的比例。在多类别分类中，准确率的定义如下：
$$
准确率 = \frac{正确的预测数}{样本总数}
$$
​        在二元分类中，准确率的定义如下：
$$
准确率 = \frac{正例数 + 负例数}{样本总数}
$$
请参阅[正例]()与[负例]()

### 激活函数 (Activation Function)

​        一种函数（例如ReLU或S型函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值）并将其传递给下一层。

### AdaGrad

​        一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效的为每个参数指定独立的学习速率。如需查看完整的解释，请参阅[论文]()。

### ROC曲线下面积 (AUC，Area Under the ROC Curve)

​        一种会考虑所有可能分类阈值的评估指标。

​        ROC曲线下面积是，对于随机选择的正类别样本确实为正类别，以及随机选择负类别样本为正类别，分类器确信前者的概率。



## B

### 反向传播算法 (Back Progpagation)

​        在神经网络上执行梯度下降法的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数。

### 基准 (baseline)

​        一种简单的模型或启发法，用做比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。

### 批次 (batch)

​        模型训练一次迭代（即一次梯度更新）中使用的样本集。

### 批次大小 (batch size)

​        一个批次中的样本数。例如：SGD的批次大小为1，而小批次的大小通常介于10到1000之间。批次大小在训练和推断期间通常是固定的。不过，TensorFlow允许使用动态批次大小。

### 偏差 (bias)

​        距离原点的截距或偏移。偏差（也称为偏差项）在机器学习模型中用“或”表示。例如，在下面的公式中，偏差为：



注意：不能和“预偏差”混淆。

### 二元分类 (Binary Classification)

​        一种分类任务，可输出两种互斥类别之一。例如：对电子邮件进行评估并输出“垃圾邮件”或“非垃圾邮件”的机器学习模型就是一个二元分类器。

### 分箱 (binning)

​        参阅“分桶”

### 分桶 (bucketing)

​        将一个特征（通常是连续特征）转换为多个二元特征（称为桶或箱），通常根据值区间进行转换。例如，可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于30.1 到 50.0 度之间的所有温度归入第三个分箱。



## C



### 校准层 (Calibration Layer)

​        一种预测后调整，通常是为了降低预测偏差的影响。调整后的预测和概率应与观察到的标签集的分不一致。

### 候选采样 (Candidate Sampling)

​        一种训练时进行的优化，会使用某种函数（例如softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。例如，如果某个样本的标签为“小猎犬”和“狗”，则候选采用将针对“小猎犬”和“狗”类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。这种采样基于的想法是，只要正类别始终得到适当的正增强，负类别就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。

### 分类数据 (Categorical Data)

​        一种特征，拥有一组离散的可能值。以某个名为 house style 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 Tudor，ranch，coloinal。通过将 house style 表示成分类数据，相应模型可以学习 Tudor，ranch 和 colonial 分别对房价的影响。

​        有时，离散集中的值是互斥的，只能将其中一个值应用于制定样本。例如，car maker 分类特征可能只允许一个样本有一个值（Toyota）。在其它情况下，则可以应用多个值。一辆车可能会被喷多种不同颜色，因此，car color 分类特征可能允许单个样本具有多个值（例如 red 和 white ）。

​        **分类特正有时称为离散特征**。与数值数据相对。

### 形心 (Centroid)

​        聚类的中心，由 K-menas 或 K-median 算法决定。例如，如果 k 为 3，则 K-means 或 K-media 算法会找出 3 个形心。

### 检查点 (Check Point)

​        一种数据，用于捕捉模型变量在特定时间的状态。借助检查点，可以导出模型权重，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，图本身不包含在检查点中。

### 类别 (Class)

​        为标签枚举的一组目标之中的一个。例如，在检测垃圾邮件的二元分类模型中，两种类别分别是“垃圾邮件”和“非垃圾邮件”。在识别狗品种的多类别分类模型中，类别可以是“贵宾犬”、“小猎犬”、“哈巴犬”等等。

### 分类不平衡的数据集 (Class-imbalanced Data Set)

​        一种二元分类问题。在此类问题中，两种类别的标签再出现频率方面具有很大的差距。例如，在某个疾病数据集中，0.0001的样本具有正类别标签，0.9999的样本具有负类别标签，这就属于分类不平衡问题；但在某个足球比赛预测器中， 0.51 的样本的标签为其中一个球队赢，0.49 的样本的标签为另一个球队赢，这就不属于分类不平衡问题。

### 分类模型 (Classisfication Model)

​        一种机器学习模型，用于区分两种或多种离散类别。例如，某个自然语言处理分类模型可以确定输入的句子是法语、西班牙语还是意大利语。请与回归模型进行比较。

###  分类阈值 (Classfication Threshold)

​        一种标量条件，应用于模型预测的得分，旨在将正类别与负类别区分开。将逻辑回归结果映射到二元分类时使用。以某个逻辑回归模型为例，该模型用于确定指定电子邮件是垃圾邮件的概率。如果分类阈值为 0.9， 那么逻辑回归值高于 0.9 的电子邮件被归类为“垃圾邮件”，低于 0.9 的被归为“非垃圾邮件”。

### 聚类 (Clustering)



### 协同过滤 (Collaborative Filtering)



### 混淆矩阵 (Confusion Matrix)